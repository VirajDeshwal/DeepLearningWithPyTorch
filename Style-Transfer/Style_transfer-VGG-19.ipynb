{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the VGG-19(features)\n",
    "\n",
    "- vgg19.features, which are all Conv2d and pooling \n",
    "- vgg19.classifier,  which are all three linear, classifier layers in the end.\n",
    "\n",
    "We will load the features portion, and \"freeze\" the weights below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /Users/SitchAI/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 574673361/574673361 [00:41<00:00, 13744989.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the features\n",
    "\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "#Freeze all VGG parameters since we are only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the content and Style Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to load the image\n",
    "\n",
    "def load_image(img_path,max_size=400, shape=None):\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    #Large images will slow down the process\n",
    "    if max(image.size) > image_size:\n",
    "        size = max_size\n",
    "        \n",
    "    else:\n",
    "        size = max(image.size)\n",
    "        \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Resize(size),\n",
    "                                   transforms.Normalize((0.485,0.456,0.406),\n",
    "                                                       (0.229,0.224,0.225))])\n",
    "    \n",
    "    #discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = transform(image)[:3,:,:].unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IMAGE_FOLDER'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3db8d21613a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load in content and style image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IMAGE_FOLDER'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Resize style to match content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Style_Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a002bf71762a>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(img_path, max_size, shape)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#Large images will slow down the process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IMAGE_FOLDER'"
     ]
    }
   ],
   "source": [
    "# load in content and style image\n",
    "content = load_image('IMAGE_FOLDER').to(device)\n",
    "\n",
    "#Resize style to match content\n",
    "style = load_image('Style_Image', shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for un-normalizing an image\n",
    "#and converting it from a tensor image to a NumPy image for display\n",
    "\n",
    "def im_convert(tensor):\n",
    "    '''Display a tensor as an image'''\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229,0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0,1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0ecedb2f2eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#content and style ims side-by-side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Display the image\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20,10))\n",
    "\n",
    "#content and style ims side-by-side\n",
    "\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace)\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace)\n",
      "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### VGG-19 Layers\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of layers\n",
    "'''\n",
    "Taking only those conv2d layers into the considering for which depth is increasing.\n",
    "eg. Conv2d(64,128)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    if layers is None:\n",
    "        layers = {\n",
    "            '0':'conv1_1',\n",
    "            '5':'conv2_1',\n",
    "            '10':'conv3_1',\n",
    "            '19':'conv4_1',\n",
    "            '21':'conv4_2', #content representation\n",
    "            '25':'conv5_1'\n",
    "            \n",
    "        }\n",
    "        \n",
    "    features = {}\n",
    "    \n",
    "    x = image\n",
    "    \n",
    "    #model._modules is a dict holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] =x\n",
    "        \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram Matrix\n",
    "The output of every convolutional layer is a Tensor with dimensions associated with the batch_size, a depth, d and some height and width (h, w). The Gram matrix of a convolutional layer can be calculated as follows:\n",
    "\n",
    "- Get the depth, height, and width of a tensor using batch_size, d, h, w = tensor.size\n",
    "- Reshape that tensor so that the spatial dimensions are flattened\n",
    "- Calculate the gram matrix by multiplying the reshaped tensor by it's transpose\n",
    "\n",
    "Note: You can multiply two matrices using torch.mm(matrix1, matrix2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \n",
    "    #get the batch_size, depth, height, and width of the tensor\n",
    "    \n",
    "    _,d,h,w = tensor.size()\n",
    "    \n",
    "    #reshape the tensor so we're multiplying the feature for each channel\n",
    "    tensor = tensor.view(d , h*w)\n",
    "    \n",
    "    #calculate the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3b5c0a78d7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Get content and style feature only once before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcontent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstyle_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "#putting all together\n",
    "\n",
    "#Get content and style feature only once before training\n",
    "\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "\n",
    "#calculate the gram matrices for each layer of our style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# create a third \"target\" image and prep it for change\n",
    "# it is a good idea to start off with the target as a copy of our *content* image\n",
    "# then iteratively change its style\n",
    "\n",
    "target = content.clone().require_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Weights\n",
    "#### Individual Layer Style Weights\n",
    "Below, you are given the option to weight the style representation at each relevant layer. It's suggested that you use a range between 0-1 to weight these layers. By weighting earlier layers (conv1_1 and conv2_1) more, you can expect to get larger style artifacts in your resulting, target image. Should you choose to weight later layers, you'll get more emphasis on smaller features. This is because each layer is a different size and together they create a multi-scale style representation!\n",
    "\n",
    "#### Content and Style Weight\n",
    "Just like in the paper, we define an alpha (content_weight) and a beta (style_weight). This ratio will affect how stylized your final image is. It's recommended that you leave the content_weight = 1 and set the style_weight to achieve the ratio you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights of each style layer\n",
    "# weighting earlier layers more will result in *larger* style artifacts\n",
    "# notice we are excluding `conv4_2` our content representation\n",
    "\n",
    "style_weights = {'conv1_1':1.,\n",
    "                 'conv2_1':0.75,\n",
    "                 'conv3_1':0.2,\n",
    "                 'conv4_1':0.2,\n",
    "                 'conv5_1':0.2}\n",
    "\n",
    "content_weight = 1 #alpha\n",
    "style_weight = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Target & Calculating Losses\n",
    "You'll decide on a number of steps for which to update your image, this is similar to the training loop that you've seen before, only we are changing our target image and nothing else about VGG19 or any other image. Therefore, the number of steps is really up to you to set! I recommend using at least 2000 steps for good results. But, you may want to start out with fewer steps if you are just testing out different weight values or experimenting with different images.\n",
    "\n",
    "Inside the iteration loop, you'll calculate the content and style losses and update your target image, accordingly.\n",
    "\n",
    "#### Content Loss\n",
    "The content loss will be the mean squared difference between the target and content features at layer conv4_2. This can be calculated as follows:\n",
    "\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "#### Style Loss\n",
    "The style loss is calculated in a similar way, only you have to iterate through a number of layers, specified by name in our dictionary style_weights.\n",
    "\n",
    "You'll calculate the gram matrix for the target image, target_gram and style image style_gram at each of these layers and compare those gram matrices, calculating the layer_style_loss. Later, you'll see that this value is normalized by the size of the layer.\n",
    "\n",
    "#### Total Loss\n",
    "Finally, you'll create the total loss by adding up the style and content losses and weighting them with your specified alpha and beta!\n",
    "Intermittently, we'll print out this loss; don't be alarmed if the loss is very large. It takes some time for an image's style to change and you should focus on the appearance of your target image rather than any loss value. Still, you should see that this loss decreases over some number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0de4958f580d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#iteration hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "# for displaying the target image, intermittently\n",
    "\n",
    "show_every = 100\n",
    "\n",
    "#iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr = 0.001)\n",
    "steps = 2000\n",
    "\n",
    "for ii in range(1, steps+1):\n",
    "    \n",
    "    #get the feature from the target image\n",
    "    target_features = get_features(target, vgg)\n",
    "    \n",
    "    #the content loss\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "    \n",
    "    #the style loss\n",
    "    #intializing to 0\n",
    "    \n",
    "    style_loss = 0\n",
    "    \n",
    "    #add to it for each layer's gram matrix loss\n",
    "    for layer in style_weights:\n",
    "        \n",
    "        #get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _,d,h,w = target_feature.shape\n",
    "        \n",
    "        #get the \"style\" style representation\n",
    "        \n",
    "        style_gram = style_gram[layer]\n",
    "        \n",
    "        # the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "    # calculate the *total* loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    \n",
    "    # update your target image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # display intermediate images and print the loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6b0bec218649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# display content and final, target image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAJDCAYAAACPEUSwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGltJREFUeJzt3X/o7fdd2PHnq4lRVmsd5gqSHyZj6Wqog3aXrEOYHe1G2j+SP3SSQNFKacAtMmYRMhxV4l+dzIGQrWZYqgWb1v4hF4xkoJWCmJJbOkuTErmLXXOj0Fhr/iltzPbeH9+v49vbpPf05vs99+SexwMOnB8fznnDm+/NK8/v53y+s9YKAAAAgP32qsu9AAAAAAAuP5EIAAAAAJEIAAAAAJEIAAAAgEQiAAAAABKJAAAAAGiDSDQzH5yZL83M517i9ZmZX5uZczPz2Zl50/EvEwBgv5jBAIBt2+RMog9Vt3+L199e3XJ4u6f6by9/WQAAe+9DmcEAgC26aCRaa32y+utvccid1W+tA49W3zszP3BcCwQA2EdmMABg247jmkTXVU8feXz+8DkAAE6OGQwAOFZXb/PDZuaeDk6H7tWvfvU/ef3rX7/NjwcAtujTn/70X621Tl3udWAGA4B98nJmsOOIRM9UNxx5fP3hc99krfVg9WDV6dOn19mzZ4/h4wGAXTQz//tyr+EKZwYDAL7Jy5nBjuPrZmeqnzz8Cxtvrp5ba/3lMbwvAAAvzQwGAByri55JNDMfqd5SXTsz56tfrL6jaq31gerh6h3Vueqr1U+f1GIBAPaFGQwA2LaLRqK11t0XeX1V//bYVgQAgBkMANi64/i6GQAAAACvcCIRAAAAACIRAAAAACIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAG0YiWbm9pl5cmbOzcx9L/L6jTPziZn5zMx8dmbecfxLBQDYL2YwAGCbLhqJZuaq6oHq7dWt1d0zc+sFh/3H6mNrrTdWd1X/9bgXCgCwT8xgAMC2bXIm0W3VubXWU2ut56uHqjsvOGZV33N4/7XVXxzfEgEA9pIZDADYqqs3OOa66ukjj89X//SCY36p+h8z87PVq6u3HcvqAAD2lxkMANiq47pw9d3Vh9Za11fvqD48M9/03jNzz8ycnZmzzz777DF9NADA3jKDAQDHZpNI9Ex1w5HH1x8+d9S7q49VrbX+pPqu6toL32it9eBa6/Ra6/SpU6cubcUAAPvBDAYAbNUmkeix6paZuXlmrungoohnLjjmi9Vbq2bmhzoYUPyaCgDg0pnBAICtumgkWmu9UN1bPVJ9voO/oPH4zNw/M3ccHvbe6j0z86fVR6p3rbXWSS0aAOBKZwYDALZtkwtXt9Z6uHr4gufed+T+E9WPHO/SAAD2mxkMANim47pwNQAAAACvYCIRAAAAACIRAAAAACIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAG0YiWbm9pl5cmbOzcx9L3HMT8zMEzPz+Mz89vEuEwBg/5jBAIBtuvpiB8zMVdUD1b+szlePzcyZtdYTR465pfoP1Y+stb4yM99/UgsGANgHZjAAYNs2OZPoturcWuuptdbz1UPVnRcc857qgbXWV6rWWl863mUCAOwdMxgAsFWbRKLrqqePPD5/+NxRr6teNzN/PDOPzsztx7VAAIA9ZQYDALbqol83+zbe55bqLdX11Sdn5ofXWn9z9KCZuae6p+rGG288po8GANhbZjAA4NhscibRM9UNRx5ff/jcUeerM2utv11r/Xn1Zx0MLN9grfXgWuv0Wuv0qVOnLnXNAAD7wAwGAGzVJpHoseqWmbl5Zq6p7qrOXHDM73bwG6xm5toOTn1+6hjXCQCwb8xgAMBWXTQSrbVeqO6tHqk+X31srfX4zNw/M3ccHvZI9eWZeaL6RPXza60vn9SiAQCudGYwAGDbZq11WT749OnT6+zZs5flswGAkzczn15rnb7c6+AbmcEA4Mr2cmawTb5uBgAAAMAVTiQCAAAAQCQCAAAAQCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAAaMNINDO3z8yTM3NuZu77Fsf92MysmTl9fEsEANhPZjAAYJsuGolm5qrqgert1a3V3TNz64sc95rq31WfOu5FAgDsGzMYALBtm5xJdFt1bq311Frr+eqh6s4XOe6Xq/dXXzvG9QEA7CszGACwVZtEouuqp488Pn/43P83M2+qblhr/d4xrg0AYJ+ZwQCArXrZF66emVdVv1q9d4Nj75mZszNz9tlnn325Hw0AsLfMYADAcdskEj1T3XDk8fWHz/2d11RvqP5oZr5Qvbk682IXTlxrPbjWOr3WOn3q1KlLXzUAwJXPDAYAbNUmkeix6paZuXlmrqnuqs783YtrrefWWteutW5aa91UPVrdsdY6eyIrBgDYD2YwAGCrLhqJ1lovVPdWj1Sfrz621np8Zu6fmTtOeoEAAPvIDAYAbNvVmxy01nq4eviC5973Ese+5eUvCwAAMxgAsE0v+8LVAAAAALzyiUQAAAAAiEQAAAAAiEQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAtGEkmpnbZ+bJmTk3M/e9yOs/NzNPzMxnZ+YPZuYHj3+pAAD7xQwGAGzTRSPRzFxVPVC9vbq1untmbr3gsM9Up9da/7j6ePWfjnuhAAD7xAwGAGzbJmcS3VadW2s9tdZ6vnqouvPoAWutT6y1vnr48NHq+uNdJgDA3jGDAQBbtUkkuq56+sjj84fPvZR3V7//chYFAIAZDADYrquP881m5p3V6epHX+L1e6p7qm688cbj/GgAgL1lBgMAjsMmZxI9U91w5PH1h899g5l5W/UL1R1rra+/2ButtR5ca51ea50+derUpawXAGBfmMEAgK3aJBI9Vt0yMzfPzDXVXdWZowfMzBurX+9gOPnS8S8TAGDvmMEAgK26aCRaa71Q3Vs9Un2++tha6/GZuX9m7jg87Feq765+Z2b+58yceYm3AwBgA2YwAGDbNrom0Vrr4erhC55735H7bzvmdQEA7D0zGACwTZt83QwAAACAK5xIBAAAAIBIBAAAAIBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAbRqKZuX1mnpyZczNz34u8/p0z89HD1z81Mzcd90IBAPaNGQwA2KaLRqKZuap6oHp7dWt198zcesFh766+stb6h9V/qd5/3AsFANgnZjAAYNs2OZPoturcWuuptdbz1UPVnRccc2f1m4f3P169dWbm+JYJALB3zGAAwFZtEomuq54+8vj84XMvesxa64Xquer7jmOBAAB7ygwGAGzV1dv8sJm5p7rn8OHXZ+Zz2/x8NnJt9VeXexF8A3uye+zJbrIvu+cfXe4FcMAMtvP8+7Wb7MvusSe7yb7snkuewTaJRM9UNxx5fP3hcy92zPmZubp6bfXlC99orfVg9WDVzJxda52+lEVzcuzL7rEnu8ee7Cb7sntm5uzlXsMrnBlsT9iT3WRfdo892U32Zfe8nBlsk6+bPVbdMjM3z8w11V3VmQuOOVP91OH9H6/+cK21LnVRAACYwQCA7bromURrrRdm5t7qkeqq6oNrrcdn5v7q7FrrTPUb1Ydn5lz11x0MMQAAXCIzGACwbRtdk2it9XD18AXPve/I/a9V//rb/OwHv83j2Q77snvsye6xJ7vJvuwee/IymcH2hj3ZTfZl99iT3WRfds8l78k4IxkAAACATa5JBAAAAMAV7sQj0czcPjNPzsy5mbnvRV7/zpn56OHrn5qZm056Tftugz35uZl5YmY+OzN/MDM/eDnWuW8uti9HjvuxmVkz4y8InLBN9mRmfuLw5+Xxmfntba9xH23wb9iNM/OJmfnM4b9j77gc69wnM/PBmfnSS/1Z9Tnwa4d79tmZedO217iPzGC7xwy2e8xfu8kMtnvMX7vnxOavtdaJ3Tq4yOL/qv5BdU31p9WtFxzzb6oPHN6/q/roSa5p328b7sm/qP7e4f2fsSe7sS+Hx72m+mT1aHX6cq/7Sr5t+LNyS/WZ6u8fPv7+y73uK/224b48WP3M4f1bqy9c7nVf6bfqn1dvqj73Eq+/o/r9aqo3V5+63Gu+0m9msN27mcF272b+2s2bGWz3buav3byd1Px10mcS3VadW2s9tdZ6vnqouvOCY+6sfvPw/sert87MnPC69tlF92St9Ym11lcPHz5aXb/lNe6jTX5Wqn65en/1tW0ubk9tsifvqR5Ya32laq31pS2vcR9tsi+r+p7D+6+t/mKL69tLa61PdvCXtV7KndVvrQOPVt87Mz+wndXtLTPY7jGD7R7z124yg+0e89cOOqn566Qj0XXV00cenz987kWPWWu9UD1Xfd8Jr2ufbbInR727g/rIybrovhyeHnjDWuv3trmwPbbJz8rrqtfNzB/PzKMzc/vWVre/NtmXX6reOTPnO/irUD+7naXxLXy7/+3h5TOD7R4z2O4xf+0mM9juMX+9Ml3S/HX1iS2HV7yZeWd1uvrRy72WfTczr6p+tXrXZV4K3+jqDk53fksHv+395Mz88Frrby7rqri7+tBa6z/PzD+rPjwzb1hr/d/LvTCATZjBdoP5a6eZwXaP+esKcdJnEj1T3XDk8fWHz73oMTNzdQenpn35hNe1zzbZk2bmbdUvVHestb6+pbXts4vty2uqN1R/NDNf6OA7pWdcPPFEbfKzcr46s9b627XWn1d/1sHAwsnZZF/eXX2saq31J9V3VdduZXW8lI3+28OxMoPtHjPY7jF/7SYz2O4xf70yXdL8ddKR6LHqlpm5eWau6eCiiGcuOOZM9VOH93+8+sN1eJUlTsRF92Rm3lj9egfDie/3bse33Je11nNrrWvXWjettW7q4DoFd6y1zl6e5e6FTf79+t0OfoPVzFzbwanPT21zkXtok335YvXWqpn5oQ6GlGe3ukoudKb6ycO/svHm6rm11l9e7kVd4cxgu8cMtnvMX7vJDLZ7zF+vTJc0f53o183WWi/MzL3VIx1cEf2Da63HZ+b+6uxa60z1Gx2cinaug4su3XWSa9p3G+7Jr1TfXf3O4fUrv7jWuuOyLXoPbLgvbNGGe/JI9a9m5onq/1Q/v9byW/gTtOG+vLf67zPz7zu4iOK7/I/vyZqZj3QwrF97eC2CX6y+o2qt9YEOrk3wjupc9dXqpy/PSveHGWz3mMF2j/lrN5nBdo/5azed1Pw19g0AAACAk/66GQAAAACvACIRAAAAACIRAAAAACIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAADV/wP27mot72NFfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display content and final, target image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
